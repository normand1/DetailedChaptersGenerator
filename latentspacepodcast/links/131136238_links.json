{"64 zetabytes": "https://healthit.com.au/how-big-is-the-internet-and-how-do-we-measure-it/#:~:text=In%202020%2C%20the%20amount%20of,and%20consumed%20on%20the%20web.", "UGC": "https://blog.hootsuite.com/user-generated-content-ugc/", "podcast prep notes": "https://github.com/swyxio/ai-notes/blob/main/Resources/DATASETS.md", "The Token Crisis paper": "https://arxiv.org/abs/2305.13230", "Ilya Sutskever on datasets": "https://overcast.fm/+-yu8j7I5U/21:00", "OpenAI Tokenizer": "https://platform.openai.com/tokenizer", "Kaplan Scaling Laws Lecture": "https://www.youtube.com/watch?v=sNfkZFVm_xs", "Chinchilla Paper": "https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training", "Sasha Rush\u2019s Tweet": "https://twitter.com/srush_nlp/status/1633509903611437058", "Karpathy\u2019s Build Conference Presentation": "https://twitter.com/altryne/status/1661236778458832896", "LIMA Paper": "https://arxiv.org/abs/2305.11206", "Phi-1 by Microsoft": "https://the-decoder.com/microsofts-tiny-phi-1-language-model-shows-the-importance-of-data-quality-in-ai-training/ from MSR", "Washington Post Article on datasets": "https://archive.is/YzguI", "Our episode with Jonathan Frankle": "https://www.latent.space/p/mosaic-mpt-7b#details", "Our episode with Mike Conover": "https://www.latent.space/p/mike-conover#details", "BloombergGPT": "https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/", "Overview": "https://commoncrawl.org/2013/08/a-look-inside-common-crawls-210tb-2012-web-corpus/", "https://stablediffusionlitigation.com/": "https://stablediffusionlitigation.com/", "https://haveibeentrained.com/": "https://haveibeentrained.com/", "https://githubcopilotlitigation.com/": "https://githubcopilotlitigation.com/", "https://twitter.com/moyix/status/1662131770463072257": "https://twitter.com/moyix/status/1662131770463072257", "OpenAI Opt Out Process": "https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq", "Check if you\u2019re in The Stack": "https://huggingface.co/spaces/bigcode/in-the-stack", "Deduplicating Training Data Makes Language Models Better": "https://arxiv.org/abs/2107.06499", "Deduplicating Training Data Mitigates Privacy Risks in Language Models": "https://arxiv.org/abs/2202.06539", "CodeForces example": "https://codeforces.com/blog/entry/113910", "Share this episode": "https://www.latent.space/p/datasets-101?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNzgyNTk4MCwicG9zdF9pZCI6MTMxMTM2MjM4LCJpYXQiOjE2OTA3MzgzOTAsImV4cCI6MTY5MzMzMDM5MCwiaXNzIjoicHViLTEwODQwODkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.QnTVOczf7mmloLuclQA6HUfRBe8SDNtly_lKp_asApQ&utm_campaign=CTA_3"}