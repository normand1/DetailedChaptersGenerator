<p><em>Thanks to the over 1m people that have checked out </em><em>the Rise of the AI Engineer</em><em>. It’s a long July 4 weekend in the US, and we’re celebrating with a podcast feed swap!</em></p><p>We’ve been big fans of Nathan Labenz and Erik Torenberg’s work at <a href="https://www.cognitiverevolution.ai/" target="_blank">the Cognitive Revolution podcast</a> for a while, which started around the same time as we did and has done an incredible job of hosting discussions with top researchers and thinkers in the field, with a wide range of topics across <a href="https://www.cognitiverevolution.ai/e6-the-computer-vision-revolution-with-junnan-li-and-dongxu-li-of-blip-and-blip2/" target="_blank">computer vision</a> (a special focus thanks to Nathan’s work at Waymark), <a href="https://www.cognitiverevolution.ai/e11-openais-gpt-4-discussion-with-nathan-labenz-and-erik-torenberg/" target="_blank">GPT-4</a> (with exceptional insight due to Nathan’s time on the GPT-4 “<a href="https://twitter.com/labenz/status/1660765244363350018" target="_blank">red team</a>”), healthcare/medicine/biotech (<a href="https://www.cognitiverevolution.ai/e24-the-ai-revolution-in-medicine-with-dr-isaac-kohane-of-harvard-medical-school/" target="_blank">Harvard Medical School</a>, <a href="https://www.cognitiverevolution.ai/e27-googles-med-palm-and-med-palm2-with-vivek-natarajan/" target="_blank">Med-PaLM</a>, <a href="https://www.cognitiverevolution.ai/e38-the-virtual-biopsy-revolution-with-dr-tanishq-mathew-abraham-part-2-of-2/" target="_blank">Tanishq Abraham</a>, <a href="https://www.cognitiverevolution.ai/e25-revolutionizing-patient-care-with-neal-khosla-of-curai-health/" target="_blank">Neal Khosla</a>), investing and tech strategy (<a href="https://www.cognitiverevolution.ai/e21-vc-insights-on-investing-in-artificial-intelligence-with-sarah-guo-and-elad-gil-of-no-priors-podcast/" target="_blank">Sarah Guo, Elad Gil</a>, <a href="https://www.cognitiverevolution.ai/e35-stability-ais-emad-mostaque-and-slow-ventures-sam-lessin-discuss-investing-in-ai/" target="_blank">Emad Mostaque, Sam Lessin</a>), <a href="https://www.cognitiverevolution.ai/e26-bonus-episode-connor-leahy-on-agi-gpt-4-and-cognitive-emulation-w-fli-podcast/" target="_blank">safety</a> and <a href="https://www.cognitiverevolution.ai/e16-pausing-the-ai-revolution-with-technologist-jaan-tallinn/" target="_blank">policy</a>, <a href="https://www.cognitiverevolution.ai/e23-scouting-the-ai-revolution-with-robert-scoble-and-bens-bites-creator-ben-tossell/" target="_blank">curators</a> and <a href="https://www.cognitiverevolution.ai/e22-helping-businesses-use-ai-with-rachel-woods-of-the-ai-exchange/" target="_blank">influencers</a> and exceptional AI founders (<a href="https://www.cognitiverevolution.ai/e34-the-consumer-rights-revolution-with-joshua-browder-of-donotpay/" target="_blank">Josh Browder</a>, <a href="https://www.cognitiverevolution.ai/e3-the-empathy-revolution-with-eugenia-kuyda-of-replika/" target="_blank">Eugenia Kuyda</a>, <a href="https://www.cognitiverevolution.ai/e9-the-ai-assistant-revolution-with-flo-crivello-of-lindyai/" target="_blank">Flo Crivello</a>, <a href="https://www.cognitiverevolution.ai/e1-the-pixel-revolution-with-playgroundais-suhail-doshi/" target="_blank">Suhail Doshi</a>, <a href="https://www.cognitiverevolution.ai/e14-the-reasoning-revolution-with-oughts-jungwon-byun-and-andreas-stuhlmuller/" target="_blank">Jungwon Byun</a>, <a href="https://www.cognitiverevolution.ai/e20-the-great-implementation-with-raza-habib-ceo-of-humanloop/" target="_blank">Raza Habib</a>, <a href="https://www.cognitiverevolution.ai/e10-the-ai-voice-revolution-with-mahmoud-felfel-of-playht/" target="_blank">Mahmoud Felfel</a>, <a href="https://www.cognitiverevolution.ai/e29-the-ai-chip-revolution-with-andrew-feldman-of-cerebras/" target="_blank">Andrew Feldman</a>, <a href="https://www.cognitiverevolution.ai/e19-the-ai-agent-revolution-with-matt-welsh-of-fixieai/" target="_blank">Matt Welsh</a>, <a href="https://www.cognitiverevolution.ai/e5-the-embedding-revolution-anton-troynikov-on-chroma-stable-attribution-and-future-of-ai/" target="_blank">Anton Troynikov</a>, <a href="https://www.cognitiverevolution.ai/e7-in-search-of-truth-with-aravind-srinivas-of-perplexity-ai/" target="_blank">Aravind Srinivas</a>). </p><p>If Latent Space is for <a href="https://www.latent.space/p/ai-engineer" target="_blank">AI Engineers</a>, then Cognitive Revolution covers the much broader field of AI in tech, business and society at large, with a longer runtime to go deep on research papers like TinyStories. We hope you love this episode as much as we do, and check out CogRev wherever fine podcasts are sold!</p><p><em>Subscribe to the Cognitive Revolution on:</em></p><p>* <em>Website</em></p><p>* <em>Apple Podcasts</em></p><p>* <em>Spotify</em></p><p>* <em>Youtube</em></p><p>Good Data is All You Need</p><p>The work of Ronen and Yuanzhi echoes a broader theme emerging in the midgame of 2023: </p><p>* Falcon-40B (trained on 1T tokens) <a href="https://twitter.com/johannes_hage/status/1662119769045188610" target="_blank">outperformed</a> LLaMA-65B (trained on 1.4T tokens), primarily due to <a href="https://arxiv.org/abs/2306.01116" target="_blank">the RefinedWeb Dataset</a> that runs CommonCrawl through extensive preprocessing and cleaning in their MacroData Refinement pipeline. </p><p>* UC Berkeley LMSYS’s Vicuna-13B is <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank">near GPT-3.5/Bard quality</a> at a tenth of their size, thanks to fine-tuning from 70k user-highlighted ChatGPT conversations (indicating some amount of quality). </p><p>* <a href="https://www.latent.space/p/reza-shabani#details" target="_blank">Replit’s finetuned 2.7B model outperforms the 12B OpenAI Codex model</a> based on HumanEval, thanks to high quality data from Replit users</p><p>The path to smaller models leans on better data (and tokenization!), whether from cleaning, from user feedback, or from <a href="https://twitter.com/karpathy/status/1671587087542530049?s=20" target="_blank">synthetic data generation</a>, i.e. finetuning high quality on outputs from larger models. TinyStories and Phi-1 are the strongest new entries in that line of work, and we hope you’ll pick through the show notes to read up further.</p><p></p><p>Show Notes</p><p>* TinyStories (Apr 2023)</p><p>* Paper: <a href="https://arxiv.org/abs/2305.07759" target="_blank">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></p><p>* <a href="https://www.youtube.com/watch?v=iNhrW0Nt7zs&#38;t=538s" target="_blank">Internal presentation with Sebastien Bubeck at MSR</a></p><p>* <a href="https://twitter.com/EldanRonen/status/1658321669407248387?s=20" target="_blank">Twitter thread from Ronen Eldan</a></p><p>* Will future LLMs be based almost entirely on synthetic training data? In a new paper, we introduce TinyStories, a dataset of short stories generated by GPT-3.5&4. We use it to train tiny LMs (< 10M params) that produce fluent stories and exhibit reasoning.</p><p>* Phi-1 (Jun 2023)</p><p>* Paper: <a href="https://arxiv.org/abs/2306.11644" target="_blank">Textbooks are all you need</a> (<a href="https://news.ycombinator.com/item?id=36413768" target="_blank">HN discussion</a>)</p><p>* <a href="https://twitter.com/SebastienBubeck/status/1671326369626853376" target="_blank">Twitter announcement from Sebastien Bubeck</a>:</p><p>* phi-1 achieves 51% on HumanEval w. only 1.3B parameters & 7B tokens training dataset and 8 A100s x 4 days = <a href="https://twitter.com/teortaxesTex/status/1673108450941759490?s=20" target="_blank">800 A100-hours</a>. Any other >50% HumanEval model is >1000x bigger (e.g., WizardCoder from last week is 10x in model size and 100x in dataset size).</p><p></p> <br /><br />Thank you for subscribing. <a href="https://www.latent.space/p/cogrev-tinystories?utm_source=substack&#38;utm_medium=podcast&#38;utm_content=share&#38;action=share&#38;token=eyJ1c2VyX2lkIjoxNzgyNTk4MCwicG9zdF9pZCI6MTMyMzk5MzMxLCJpYXQiOjE2OTA3MzgzOTAsImV4cCI6MTY5MzMzMDM5MCwiaXNzIjoicHViLTEwODQwODkiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.4Gvbe3X4MVT2fntsKERv_6BE8kxUGA8akfnM2dEUcwA&#38;utm_campaign=CTA_3">Share this episode</a>.