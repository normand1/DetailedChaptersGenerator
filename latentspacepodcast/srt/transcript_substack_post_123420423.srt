32
00:00:00,000 --> 00:01:30,000
*  Introductions

33
00:01:30,000 --> 00:03:30,000
*  Itamar’s background and previous startups

34
00:03:30,000 --> 00:06:00,000
*  Vision for Codium AI: reaching “zero bugs”

35
00:06:00,000 --> 00:15:30,000
*  Demo of Codium AI and how it works

36
00:15:30,000 --> 00:22:30,000
*  Building on VS Code vs JetBrains

37
00:22:30,000 --> 00:27:00,000
*  Future of software development and the role of developers

38
00:27:00,000 --> 00:30:00,000
*  The vision of integrating natural language, testing, and code

39
00:30:00,000 --> 00:39:00,000
*  Benchmarking AI models and choosing the right models for different tasks

40
00:39:00,000 --> 00:43:30,000
*  Codium AI spec generation and editing

41
00:43:30,000 --> 00:52:30,000
*  Reconciling differences in languages between specs, tests, and code

42
00:52:30,000 --> 01:03:00,000
*  The Israeli tech scene and startup culture

43
01:03:00,000 --> 00:00:00,000
*  Lightning Round

55
00:00:00,000 --> 00:00:00,000
Alessio:  Hey everyone. Welcome to the Latent Space podcast. This is Alessio, Partner and CTO-in-Residence at Decibel Partners. I'm joined by my co-host, Swyx, writer and editor of Latent Space.

62
00:01:30,000 --> 00:00:00,000
Itamar: Generally speaking, like there's a lot of peer-to-peer transactions going on, like payments and, and China with QR codes. So basically if for example 5% of the scanning does not work and with our scanner we  reduce it to 4%, that's a lot of money. Could be tens of millions of dollars a day.

66
00:03:00,000 --> 00:00:00,000
Like you can go both TDD (Test-Driven Development) or classical coding. And we offer analysis, tests, whether they pass or not, we further self debug  them and make suggestions eventually helping to improve the code quality specifically on code logic testing.

70
00:04:30,000 --> 00:00:00,000
So that was the pain I always had, especially when I did have tools for that, for the hardware. Like I worked in Mellanox to be sold to Nvidia as a student, and we had formal tools, et cetera.  So that's one part.

77
00:06:00,000 --> 00:00:00,000
Alessio: Maybe talk a little bit more about the technical implementation of it. You mentioned the agent  part. You mentioned some of the model part, like what happens behind the scenes when Codium gets in your code base?

82
00:07:30,000 --> 00:00:00,000
For example, if I mutate some equation in the application code and the test finds a bug and it does that at a really high rate, like out of 100 mutation, I  find all of the 100 problems in the test. It's probably a very strong test suite.

85
00:09:00,000 --> 00:00:00,000
Logic different from performance or quality. If I did a three for in loop, like I loop three things and I can fold them with some vector like in Python or something like that. We need to get into the mind of the developer. What was the intention? Like what is the bad code? Not what is the code logic that doesn't work. It's not according to the specification. So I think like one more thing that AI could really help is help to match, like if there is some natural language description of the code, we can match it. Or if there's missing information in natural language that needs  to be asked for the AI could help asking the user.

90
00:10:30,000 --> 00:00:00,000
You'll have this small button. There's other way you can mark specific code and right click and run code. But this one is my favorite because we actually choose above which components we suggest to use code. So once I click it code, I starts analyzing this class. But not only this class, but almost everything that is  being used by the call center class.

94
00:12:00,000 --> 00:00:00,000
I wanna show you the next one, which is run all test. First, we verify that you're okay, we're gonna run it. I don't know, maybe we are connected to the environment that is currently  configured in the IDE. I don't know if it's production for some reason, or I don't know what. Then we're making sure that you're aware we're gonna run the code that and then once we run, we show if it pass or fail.

100
00:13:30,000 --> 00:00:00,000
We're making a diff now that you can apply on your code. So basically what, what we're seeing here is that  there are three main tabs, the code, the test and the code analysis. Let's call spec.

103
00:15:00,000 --> 00:00:00,000
Your code, if you don't provide it with some input is valuable, like adjacent with all information or YAMA or whatever. So you can actually add input data and the AI or model. It's actually by the way, a set of models and algorithms that will use that input to create interesting tests. And another thing is many people have some reference tests that they already made. It could be because they already made it or because they want like a very specific they have like how they imagine the test. So they just write one and then you add a reference and that will inspire all the rest of the tests. And also you can give like hints.  This is by the way plan to be like dynamic hints, like for different type of code.

111
00:16:30,000 --> 00:00:00,000
Itamar: I love that you, you mentioned that because if you go to CS undergrad you take so many courses in development, but none of them probably in testing, and it's so important. So why would you, and you don't go to Udemy or  whatever and, and do a testing course, right? Like it's, it's boring. Like people either don't do component level testing because they hate it or they do it and they hate it. And I think part of it it’s because they're missing tool to make it fun.

115
00:18:00,000 --> 00:00:00,000
And I forced it to add mocks,  the tests were deleted and now we're creating six new tests. And you see, here's the shiver me timbers, the test checks, the call successful, probably there's some joke at the end. So in this case, like even if you try to force it to mock it didn't happen because there's nothing but we might find here like stuff that it mock that really doesn't make sense because there's nothing to mock here.

122
00:19:30,000 --> 00:00:00,000
Mm-hmm. I'm, I'm like giving you this hard question later. Yeah. So if you ask ChatGPT give me an example to test a code, it might give you this bank account. It's like the one-on-one stuff, right? And one of the reasons I gave it, because it's easy to inject bugs here, that's easy to understand  anyway.

127
00:21:00,000 --> 00:00:00,000
Itamar:   First of all, I'm a relatively transparent person. Like even as a manager, I think I was like top one percentile being transparent in Alibaba. It wasn't five out of five, which is a good thing because that's extreme, but it was a good, but it also could be a bad, some people would claim it's a bad thing.

131
00:22:30,000 --> 00:00:00,000
Like at the moment we hope it to be used weekly. And that's what we're getting. And the growth is about like every two, three weeks we double the amount of weekly and downloads. It's still very early, like seven weeks. So I don't know if it'll keep that way, but we hope so. Well  actually I hope that it'll be much more double every two, three weeks maybe. Thanks to the podcast.

135
00:24:00,000 --> 00:00:00,000
And one of the things like I'm really surprised is that one team, I saw one user two weeks ago, I was so happy. And then I came yesterday and I saw 48 of that company. So what I'm trying to say to be frank is that we see more intra virality right now than inter virality. I don't see like video being shared all around Twitter. See what's going on here. Yeah. But I do see, like people share within the company, you need to use it because it's really helpful with productivity and it's something that we will work about the  inter virality.

144
00:25:30,000 --> 00:00:00,000
I think  like different models are better on different properties, for example, how obedient you are to instruction, how good you are to prompt forcing, like to format forcing. I want the results to be in a certain format or how accurate you are or how good you are in understanding code.

148
00:27:00,000 --> 00:00:00,000
Alessio: Yeah, maybe talk a little bit more about. How do I actually get all these models to work together? I think there's a lot of people that have only been exposed to Copilot so far, which is one use case, just complete what I'm writing. You're doing a lot more things here. A lot of people listening are engineers themselves, some of them build these tools, so they would love to  hear more about how do you orchestrate them, how do you decide which model the what, stuff like that.

154
00:28:30,000 --> 00:00:00,000
Maybe another model is better in forcing the, a certain format you probably saw on Twitter, et cetera. People talk about it's hard to ask model to output JSON et cetera. So basically we predefine. For different tasks, we, we use different models and I think like this is for individuals, for developers to check, try to sync, like the test that now you are working on, what is most important for you to get, you want the semantic understanding, that's most important? You want the output, like are you asking for a very specific  output?

159
00:30:00,000 --> 00:00:00,000
Understand the  properties you're aiming for and start playing with that. And only then go to train your own model.

165
00:31:30,000 --> 00:00:00,000
So  I would expect, like, for example, for that model to go. This is my I what I think to search the internet and do a certain thing. So level number three could be that I want to check that as part of this request. It uses a certain tools level five, you can add to that. I expect that it'll bring me back something like relevance and level nine it actually prints the cocktail for me I taste it and it's good. So, so I think like how I see it is like we need to have data sets similar to before and make sure that we not fine tuning the model the same way we test it. So we have one challenges that we fine tune over, right? And few challenges that we don't.

174
00:33:00,000 --> 00:00:00,000
Itamar: Yeah.  So currently the VS Code extension is leading over JetBrains. And we were for a long time and, and like when I tell you long time, it could be like two or three weeks with version oh 0.5, point x something in, in VS code, although oh 0.4 or so a jet brains, we really saw the difference in, in the how people react.

177
00:34:30,000 --> 00:00:00,000
Although JetBrains has like very nice property, when you develop extension for one of the IDEs, it usually works well for all the others, like it's one extension for PyCharm, and et cetera. I think like there's even more flexibility in the VS code. Like for example, this app is, is a React extension as opposed that it's native in the JetBrains one we're using. What I learned is that it's basically is almost like  developing Android and iOS where you wanna have a lot of the best practices where you have one backend and all the software development like best practices with it.

180
00:36:00,000 --> 00:00:00,000
And in the future we're also gonna have like teams offering of collaboration Right now if you close Codium Tab, everything is like lost except of the test code, which you, you can, like if I go back to a test suite and do open as a file, and now you have a test file with everything that you can just save, but all the goodies here it's lost. One day we're gonna have like a platform you can save all that, collaborate with people, have it part of your PR, like have suggested part of your PR. And then you wanna have some alignment. So one of the challenges, like UX/UI, when you think about a feature, it should, some way or another fit for both platforms be because you want, I think by the way, in iOS and Android, Android sometimes you don’t care about parity, but here you're talking about developers that might be on the same  team.

187
00:37:30,000 --> 00:00:00,000
So I think like traditional development. Today works like creating some spec for different companies,  different development teams. Could mean something else, could be something on Figma, something on Google Docs, something on Jira. And then usually you jump directly to code implementation. And then if you have the time or patience, or will, you do some testing.

191
00:39:00,000 --> 00:00:00,000
Let's have that in mind. So I think like, What's happening right now when you saw our demo is what I presented a few minutes ago, is that you start with an implementation and we create spec for you and test for you. And that was like a agent, like you didn't converse with it, you just  click a button.

195
00:40:30,000 --> 00:00:00,000
But you can also from spec create tests, okay? From the spec directly to tests. 

200
00:42:00,000 --> 00:00:00,000
Now the developers is the driver, okay? You'll have a lot  of like, what do you think about this? This is what you meant. Yes, no, you wanna fix the coder test, click yes or no. But you still be the driver. But there's gonna be like extreme automation on the DRY level. So that's what we're announcing, that we're aiming for as our vision and what we're providing these days in our product is the middle, is what, what you see in the middle, which is our code integrity agents working for you right now in your id, but soon also part of your Github actions, et cetera, helping you to align all these three.

204
00:43:30,000 --> 00:00:00,000
So for example, as the spec, we won't let you input Figma, but don't be surprised if in 2024 the input of the spec could be a Figma. Actually, you can see  demos of that on a pencil drawing from OpenAI and when he exposed the GPT-4. So we will have that actually.

212
00:45:00,000 --> 00:00:00,000
 Yeah. Sometimes, like, as you mentioned, sometimes people don't have time to write the test. Sometimes people don't have time to write the spec. Yeah. So sometimes you end up with things. Out of sync, you know? Yeah. Or like the implementation is moving much faster than the spec, and you need some of these agents to make the call sometimes to be like, no.

216
00:46:30,000 --> 00:00:00,000
Mm-hmm. If the singularity happens, then we're talking about this new set of problems. Let's put that aside. Like even if it happens in 2041, that's my prediction. I'm not sure like you should aim for thinking what you need to do, like, or not when the singularity happens. So I,  I would aim for mm-hmm.

220
00:48:00,000 --> 00:00:00,000
So I think like there will be like degree of diff different type developers now. If you think about it for a second, I think like it's a natural evolution. It's, it's true today as well. Like if you know really good the Linux or assembly, et cetera, you'll probably work like on LLVM Nvidia  whatever, like things like that.

228
00:49:30,000 --> 00:00:00,000
So it could create tests, run them, fix them. It's a few tests. So we really believe in that we're  building a designated agent while Auto GPT is like a swarm of agents, general agents that were supposedly you can ask, please make me rich or make me rich by increase my net worth.

233
00:49:00,000 --> 00:00:00,000
Swyx: a very small  meeting

244
00:52:30,000 --> 00:00:00,000
And that's what we specialize in Codium, but I wanna say that I'm not saying that Auto GPT won't be able to get there. Like the more tools and that going to be added, the  more prompt engineering that is dedicated for this, this idea will be added by the way, where I'm talking with Toran, that Codium, for example, would be one of the agents for Auto GPT.

249
00:57:00,000 --> 00:00:00,000
Itamar: people know? So I think like Israel is the most condensed startup per capita. I think we're number one really? Or, or startup pair square meter. I think, I think we're number one as well because of these properties actually there is a very strong community and like everyone are around, like are  working in a.

254
00:58:30,000 --> 00:00:00,000
And that's, that's why I'm here in addition to to, you know, to  me and, and participate in this amazing podcast, et cetera.

261
01:00:00,000 --> 00:00:00,000
Itamar: It's a question we're being asked a lot, like why, for example, let's go to the soft skills. I think like failing is a bad thing. Yeah. Like, okay. Like sometimes like VCs prefer to  put money on a, on an entrepreneur that failed in his first startup and actually succeeded because now that person is knowledgeable, what it mean to be, to fail and very hungry to, to succeed.

269
01:01:30,000 --> 00:00:00,000
Mm-hmm. And, and I was like going to like VCs and V P R and D is director, et cetera, and telling them, listen, we're gonna help with code logic testing and we're going to do that interactive conversation way. And they were like, no way. I even had like two saying, I won't let your silly AI get close to my code.

276
01:03:00,000 --> 00:00:00,000
It's, it's, it's not easy. Like it raises so many question even about ourself  as as human or we, like, I saw one tweet by someone that I'm thinking about like for a few years he wrote are we actually like LLMs, like in essence? So, so I think like we're trying to look into those LLMs for years. Like there, there was, like in 2014 there was already in the C N N, there was a few works.

281
01:04:30,000 --> 00:00:00,000
Okay. So why am I saying that? Because if you're a builder, I really encourage you, speak less and do more play with it. Try it for specific use cases and see what's easy to do. And then if your purpose is just like incorporating stuff and that's what you wanna do and  then do it, but don't like, tell everyone you're gonna do it before you do it, because you might find that it's actually really hard and there's a lot of problems.

287
01:06:00,000 --> 00:00:00,000
Itamar: Thank you for inviting me. It was a pleasure.

