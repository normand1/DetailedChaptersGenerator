27
00:00:00,000 --> 00:03:20,000
* Introductions 

28
00:03:20,000 --> 00:05:45,000
* Intro to Mosaic 

29
00:05:45,000 --> 00:08:45,000
* Training and Creating the Models 

30
00:08:45,000 --> 00:10:00,000
* Data Choices and the Importance of Repetition 

31
00:10:00,000 --> 00:00:00,000
* The Central Question: What Mix of Data Sets Should You Use? 

33
00:16:00,000 --> 00:19:50,000
* Flash Attention 

34
00:19:50,000 --> 00:23:00,000
* Fine-tuning for Creativity 

35
00:23:00,000 --> 00:25:15,000
* Open Source Licenses and Ethical Considerations 

36
00:25:15,000 --> 00:30:00,000
* Training Stability Enhancement 

37
00:30:00,000 --> 00:34:00,000
* Data Readiness & Training Preparation 

38
00:34:00,000 --> 00:36:00,000
* Dynamic Real-time Model Evaluation 

39
00:36:00,000 --> 00:40:15,000
* Open Science for Affordable AI Research 

40
00:40:15,000 --> 00:44:11,000
* The Open Approach 

41
00:44:11,000 --> 00:48:01,000
* The Future of Mosaic 

42
00:48:01,000 --> 00:54:00,000
* Speed and Efficiency 

43
00:54:00,000 --> 00:00:00,000
* Trends and Transformers 

46
00:00:00,000 --> 00:00:00,000
Alessio:  Hey everyone. Welcome to the Latent Space podcast. This is Alessio partner and CTO-in-Residence at Decibel Partners. I'm joined by my co-host, Swyx, writer and editor of Latent Space.

56
00:01:30,000 --> 00:00:00,000
So Jonathan, you did your BS and MS at Princeton in programming languages and then found your way into ML for your PhD at MiT where you made a real splash with the lottery ticket hypothesis in 2018, which people can check up on. I think you've done a few podcasts about it over the years, which has been highly influential, and we'll talk about sparse models at Mosaic. You have also had some side  quest. You taught programming for lawyers and you did some law and privacy stuff in, in DC and also did some cryptography stuff. Um, and you've been an assistant professor at Harvard before earning your PhD.

65
00:03:00,000 --> 00:00:00,000
Swyx: And do you use like  a different programming language or framework to do that? Or is that like..

70
00:03:20,000 --> 00:00:00,000
INTRO TO MOSAIC 

82
00:05:45,000 --> 00:00:00,000
TRAINING AND CREATING THE MODELS  

90
00:07:30,000 --> 00:00:00,000
Um, and that we usually go by the Chinchilla laws. Now for these models, we actually didn't quite do that because we wanted to make sure that people could actually run these at home and that they  were good for inference. So we trained them kind of beyond those chinchilla points so that we're almost over-training them.

99
00:08:45,000 --> 00:00:00,000
DATA CHOICES AND THE IMPORTANCE OF REPETITION 

102
00:09:00,000 --> 00:00:00,000
Um, at the end of the day, We know that you need to train these models and  lots of data, but there are a bunch of things we don't know.

107
00:10:00,000 --> 00:00:00,000
THE CENTRAL QUESTION: WHAT MIX OF DATA SETS SHOULD YOU USE? 

109
00:10:30,000 --> 00:00:00,000
Knowing they knew nothing about large language models, they just know that data goes in and it's going to affect the behavior. Um, and I was like, create a mix and they basically covered all the different trade-offs. Um, you probably want a lot of English language  text to start with. You get that from the web, but do you want it to be multilingual?

114
00:12:00,000 --> 00:00:00,000
So it's not like it memorized the data, it is just one web scrape from 2019. If you actually look at the T five paper and see how it was pre-processed, it looks very silly. Mm-hmm. They removed anything that had the word JavaScript in it because they didn't want to get like no JavaScript  warnings. They removed anything with curly braces cuz they didn't wanna get JavaScript in it.

122
00:13:30,000 --> 00:00:00,000
Jonathan: A lot of this stuff is you've got two kinds of tasks. Some of these are more of multiple choice style tasks where there is a right answer. Um, either you ask the model to spit out A, B, C, or D or you know, and if you're more  sophisticated, you look at the perplexity of each possible answer and pick the one that the model is most likely to generate.

131
00:15:00,000 --> 00:00:00,000
There's just almost no way we get ahead of time, like measuring individual dimensions. And then also particularly like, you know, at the 7B scale,  um, these models still are not super great yet at the really hard tasks, like some of the hardest tasks in MMLU and stuff. So sometimes they're barely scoring like the above kind of random chance, you know, like on really, really hard tasks.

138
00:16:00,000 --> 00:00:00,000
FLASH ATTENTION 

143
00:16:30,000 --> 00:00:00,000
Abhinav: Yeah, absolutely. We really, really liked flash attention. We, I think, had to integrate into repo even as  as early as September of last year. And it really just helps, you know, with training speed and also inference speed and we kind of bake that into model architecture.

151
00:18:00,000 --> 00:00:00,000
Now one of  the, the funny things we found is like with flash attention, it saved so much memory and like improved performance so much that even as early as I kind of last year, like we were profiling models with, with very long context lines up to like, you know, the 65 k that you seen in release, we just never really got around to using it cuz we didn't really know what we might use it for.

161
00:19:30,000 --> 00:00:00,000
Yeah. But there are a bunch of really, really good ones. There was one where, you know, it's Gatsby's funeral and then Nick starts talking to Gatsby's Ghost, and Gatsby's father shows up and, you know, then he's  at the police station with Tom. It was very plot heavy, like this is what comes next. And a bunch of that were just very Fitzgerald-esque, like, you know, beautiful writing.

163
00:19:50,000 --> 00:00:00,000
FINE-TUNING FOR CREATIVITY 

170
00:21:00,000 --> 00:00:00,000
So, you know, I've given up trying to even predict. Yeah, yeah. Until I see the data or try it, I just kind shg my shoulders and you know, you hope for the best. Bring data or else, right? Yeah,  exactly. Yeah, yeah, yeah.

180
00:22:30,000 --> 00:00:00,000
Boy, you know,  that's, that's not timing that we appreciated. And so we talked a lot internally that night about like, oh, we've had time to read the news. We've had time to take a breath. We don't really love this. Came to the conclusion that it's better to just leave it as it is now and learn the lesson for the future.

183
00:23:00,000 --> 00:00:00,000
OPEN SOURCE LICENSES AND ETHICAL CONSIDERATIONS 

188
00:24:00,000 --> 00:00:00,000
And who is to say how much is memorization by a model versus actually learning and internalizing and then. Sometimes happening to land at the right, the  same result.

198
00:25:15,000 --> 00:25:30,000
TRAINING STABILITY ENHANCEMENT 

199
00:25:30,000 --> 00:00:00,000
Swyx: Yeah. Yeah. Well, you've been very thoughtful about it. Okay. So a lot of these other ideas in terms of architecture, flash, attention, alibi, and the other data sets were contributions from the rest of the let's just call it open community of, of machine learning advancements. Uh, but Mosaic in  particular had some stability improvements to mitigate loss spikes, quote unquote, uh, which, uh, I, I took to mean, uh, your existing set of tools, uh, maybe we just co kind of covered that. I don't wanna sort of put words in your mouth, but when you say things like, uh, please enjoy my empty logbook.

207
00:27:00,000 --> 00:00:00,000
And apply some of these like, uh, interventions. So we had these kinds of preparations, like a plan B, but we didn't have to use them at all for MPT 7B training. So, that was kind of like a lucky break. And the third part of like basically getting all the way to the empty law book is having the right training infrastructure.

212
00:28:30,000 --> 00:00:00,000
Jonathan: I do wanna say that was hard one. Mm-hmm. Um, certainly this is not how things were going, you know, many months ago, hardware failures we had on calls who were, you know, getting up at two in the morning to, you know, figure out which node had died for what reason, restart the job, have to cord the node.  Um, we were seeing catastrophic loss spikes really frequently, even at the 7B scale that we're just completely derailing runs.

218
00:30:00,000 --> 00:30:00,000
Pull from a checkpoint. Yeah. Restart again on different hardware. But for now, we're certainly in a world where if anything dies, that's the end of the run and you have to go back and recover from it. 

219
00:30:00,000 --> 00:00:00,000
DATA READINESS & TRAINING PREPARATION 

227
00:31:30,000 --> 00:00:00,000
One is actually starting from one of our checkpoints, which I think very few of our customers are actually going to do, and one is starting from our configuration. You can look at our friends at Rep for that, where, you know, MPT was in progress when Refl  came to us and said, Hey, we need a 3 billion parameter model by next week on all of our data.

233
00:33:00,000 --> 00:00:00,000
And it's really funny, like a lot of times customers will be really excited about training the models, right? It's really fun to like launch shelves on hundreds of gfs, just all around. It's super fun. But then they'll be like, but wait, what are we gonna measure? Not just the training loss, right? I mean, it's gotta be more than that.

241
00:34:00,000 --> 00:00:00,000
DYNAMIC REAL-TIME MODEL EVALUATION 

244
00:34:30,000 --> 00:00:00,000
Mm-hmm. But when you ask it, uh, you know, I don't know. I think one of our prompts was to suggest games for a three-year-old and a seven-year-old. That would be fun to play. Like that was a lot more  valuable to me personally, to see how that answer evolved and changed over the course of training. So, you know, and human eval, just to clarify for folks, human human eval is an automated evaluation metric.

254
00:36:00,000 --> 00:00:00,000
We still haven't talked about efficiency and speed. Those are usually our two watch words at Mosaic, which is, you know, that's great. That says that we're  doing a lot of other cool stuff, but at the end of the day, um, you know, Cost comes first. If you can't afford it, it doesn't matter. And so, you know, getting things down cheap enough that, you know, we can monitor in real time, getting things down cheap enough that we can even do it in the first place.

256
00:36:00,000 --> 00:00:00,000
OPEN SCIENCE FOR AFFORDABLE AI RESEARCH 

262
00:37:30,000 --> 00:00:00,000
Abhinav: Yeah. And I think one of the special things about Mosaic's kind of  position as well is that we have such, so many customers all trying to train models that basically we have the incentive to like to devote all these resources and time to do this science.

269
00:39:00,000 --> 00:00:00,000
There's no cost to us in sharing what we learn with the community. Cuz really at the end of the day, we make our money off of those custom models and great infrastructure and, and putting all the pieces together. That's honestly where the Mosaic name came from. Not off of like, oh, we've got, you know, this one cool secret trick  that we won't tell you, or, you know, closing up.

274
00:40:15,000 --> 00:00:00,000
THE OPEN APPROACH 

276
00:40:30,000 --> 00:00:00,000
Obviously today, GPT four is still, you know, part of like that state-of-the-art model for a  lot of tasks. Do you think some of the innovation and kind of returning methods that we have today are enough if enough people like you guys are like running these, these research groups that are open? Or do you think we still need a step function improvement there?

280
00:42:00,000 --> 00:00:00,000
That's way beyond our means and not what we're trying to do anyway. But there's also a whole world for, you know, domain specific models, context specific models that are really specialized, proprietary, trained on your own data that can do things that you could never do with one of these big models. You can customize in crazy ways like G B T four is not gonna hit 65 K context length for a very long time, cuz they've already trained that  model and you know, they haven't even released the 32 K version yet.

287
00:43:30,000 --> 00:00:00,000
I think it's gonna slow down the pace of progress. In a lot of cases, each of these labs has a bit of a monoculture and being able to pass ideas  back and forth was a lot of what kept, you know, scientific progress moving. So it's imperative not just, you know, for the open source community and for academia, but for the progress of technology.

289
00:44:11,000 --> 00:00:00,000
THE FUTURE OF MOSAIC 

295
00:45:00,000 --> 00:00:00,000
We're going to put our best foot forward and make something really truly amazing. But there is, you know, that's something that we were reluctant to do. You know, our customers convinced us it would be good for our business. It's been wonderful for business and we are gonna put everything into this, but you know, back when grading dissent came out, I  was thinking like, or when we recorded it or focused, oh God, like focus is the most important thing.

305
00:46:30,000 --> 00:00:00,000
Of  course not. You never, like, MPT has a thousand different things we wanted to do that we never got to. So, you know, there will be future models.

312
00:48:01,000 --> 00:48:00,000
SPEED AND EFFICIENCY 

313
00:48:00,000 --> 00:00:00,000
Swyx: Amazing answer. Well, thanks. Uh, we can, we can touch on a little bit  on, uh, efficiency and speed because we, we, uh, didn't mention about that. So right now people spend between three to 10 days. You, you spend 10 days on, on mpc, seven rep spend three days. What's feasible? What's what Do you wanna get it down to?

317
00:49:30,000 --> 00:00:00,000
On top of that, you know, there's a lot of architectural applications. We're looking at ways to introduce some forms of sparsity, not necessarily like the, the, the super unstructured sparsity like lottery ticket. Um, which not that I'm sure I'm really happy to talk about. Um, but, but, um, are there ways of doing, like you  gating or like, kind of like m moe style architectures?

326
00:51:00,000 --> 00:00:00,000
Alessio: So that's making existing  things better with the, the long boy, the 60 5K context window, uh, you've doubled instead of the r.

333
00:52:30,000 --> 00:00:00,000
This is promised in a paper, the  incentives just aren't there, which is part of the reason we went with just pure quadratic attention here. Like it's known to work. We didn't have to make an approximation. There's no asterisk or caveat. This was in some sense a sheer force of will by our amazing engineers.

341
00:54:00,000 --> 00:54:00,000
TRENDS AND TRANSFORMERS 

342
00:54:00,000 --> 00:00:00,000
Swyx: Right? Um, you've famously made one.  Countertrend, uh, bet, which is, uh, you, you're actually betting that, uh, transformers will stick around for a long time. 

356
00:55:30,000 --> 00:00:00,000
Abhinav:  I think it's gonna be just like MLPs, you know, that's the only, that's the only way we can go, I think at this point, because Thelp, I, I dunno. Oh, just basically down to, to um, to linear layers.

365
00:57:00,000 --> 00:00:00,000
Jonathan: It's a little fuzzy. I mean, I think at the end of the day you have to ask what is the point of our business? Our business is not just to train a bunch of open models and give them to the world. That would, our VCs probably wouldn't be very happy if that were the case. The open  models serve our business because they're demos.

376
00:58:30,000 --> 00:00:00,000
The first slide is a, you know, a circle with a slash through it over a lottery ticket.  Um, and anyone who mentions lottery tickets, I ask to leave the room. Um, cuz you know there's other work out there. But Abhi, please feel free to dish on sparsity.

383
01:00:00,000 --> 00:00:00,000
So these models, steel is all you need. These models love unstructured  sparsity. Um, and yeah, if there were a chip and a software package that made it really, really easy to accelerate it, I bet we would be doing it at Mosaic right now. 

395
01:01:30,000 --> 00:00:00,000
You know, they can't possibly keep getting better as they make it bigger. And then GPT three came out and I was like, eh, it's slightly better at  generating text. Yeah, who cares? And you know, I've been wrong again and again and again. That. Next token prediction, making things big can produce useful models.

403
01:03:00,000 --> 00:00:00,000
And so the most interesting I think I've found is that if you go to the subreddits, you know, for those communities and you see like how they  talk about and think about their like AI friends and like these characters, it's, it's, it's like out of a science fiction book, like I would never expect this to be like reality.

410
01:04:30,000 --> 00:00:00,000
I, you know, open ai has shown us that there is one path to getting these incredible capabilities that is scale. I hope that's not the only path. I hope there are lots of ways of getting there. There's better modeling, there are better algorithms. I hate the neuroscience metaphors, but in some sense, our existence and our brains are, you know, evidence that there is at least one other way to get to these kinds of incredible capabilities that doesn't require, you know,  a trillion parameters and megawatts and megawatts and gazillions of dollars.

418
01:06:00,000 --> 00:00:00,000
Abhinav: And I would also say just kinda like research done in the open. I think like, you know, there's no computing with the, the open community,  right?

